{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3d83bb",
   "metadata": {},
   "source": [
    "# LangChains fake LLm good while developement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "189512d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (0.3.27)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (1.99.9)\n",
      "Requirement already satisfied: langchain_anthropic in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain_core in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (0.3.74)\n",
      "Collecting langchain_openai\n",
      "  Using cached langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (0.4.14)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_community) (2.3.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_core) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_core) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_core) (2.11.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain_core) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2025.8.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: anthropic<1,>=0.60.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langchain_anthropic) (0.64.0)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langsmith>=0.1.125->langchain_community) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/langchain_experiments/lib/python3.11/site-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
      "Using cached langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-macosx_11_0_arm64.whl (999 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.3/999.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp311-cp311-macosx_11_0_arm64.whl (285 kB)\n",
      "Installing collected packages: regex, tiktoken, langchain_openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [langchain_openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed langchain_openai-0.3.30 regex-2025.7.34 tiktoken-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community python-dotenv openai langchain_anthropic langchain_core langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35161df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.fake import FakeListLLM\n",
    "\n",
    "# Create a fake LLM that always returns the same response\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "result = fake_llm.invoke(\"Any input will return Hello\") \n",
    "print(result) # Output: Hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c998872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The financial capital of India is Mumbai.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openAIClient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = openAIClient.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Finantial capital of India?\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1562576",
   "metadata": {},
   "source": [
    "## print the reasoning with the response \n",
    "\n",
    "### example with claude model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e8ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'signature': 'ErUBCkYIBhgCIkAWa0xxxdfdjgjqdVdPAGulSMsI8lBxzKzRSiiKx3AU5jnCrJYjR/gi0bLtACvbvvhWEQjcxN/779Qj0kmh3BJtEgy+OmHb//qj8YJ7biQaDEqavPdha7tcwGQ5cCIwvpt5GBXQP5oHkJotdWKbW4enBiQYVkGtHkYOnITA2vLPKdtlqv5K8iE9kNQhYULsKh3B7pR7OIL20bar7mvL4Hlza4dydl+Agw8vJMHbmxgC', 'thinking': \"Alright, I'm being asked to design an algorithm to find the kth largest element in an unsorted array, with the optimal time complexity. I also need to analyze the time and space complexity and explain why it's optimal.\\n\\nFirst, let's explore a few approaches to solve this problem:\\n\\n1. **Sorting**: We could sort the array and then return the kth largest element. \\n   - Time Complexity: O(n log n) where n is the size of the array\\n   - Space Complexity: O(1) if we're using an in-place sorting algorithm\\n   - This is not optimal in terms of time complexity for this specific problem, because we don't need to sort the entire array.\\n\\n2. **Min-Heap**: We could use a min-heap of size k.\\n   - We iterate through the array, adding each element to the heap.\\n   - If the heap size exceeds k, we remove the smallest element.\\n   - At the end, the smallest element in the heap is the kth largest.\\n   - Time Complexity: O(n log k) where n is the size of the array\\n   - Space Complexity: O(k) for storing the heap\\n   - This is better than sorting, but still not optimal.\\n\\n3. **Max-Heap**: We could use a max-heap with all elements.\\n   - We heapify the array in O(n)\\n   - Then extract max k times, which takes O(k log n)\\n   - Time Complexity: O(n + k log n)\\n   - Space Complexity: O(1) if we're using the array itself for the heap\\n   - This is still not optimal if k is large.\\n\\n4. **QuickSelect**: This is a selection algorithm to find the kth smallest or largest element in an unordered list.\\n   - It's similar to QuickSort, but instead of recursing for both left and right sides of the pivot, we only recur for one side, the side with the kth element.\\n   - Expected Time Complexity: O(n) on average, O(n^2) in the worst case\\n   - Space Complexity: O(1) if we're using an in-place implementation\\n   - This is the optimal approach for this problem in terms of average time complexity.\\n\\nLet's implement and analyze the QuickSelect algorithm, which is the optimal solution for this problem:\\n\\nQuickSelect algorithm to find the kth largest element:\\n1. Choose a pivot element from the array.\\n2. Partition the array around the pivot (elements less than the pivot to the left, elements greater to the right).\\n3. If the pivot's position is the (n-k)th index (where n is the array size), return the pivot.\\n4. If the pivot's position is greater than the (n-k)th index, recur for the left subarray.\\n5. Otherwise, recur for the right subarray.\\n\\nNote: To find the kth largest, we're actually looking for the (n-k)th smallest element. For example, in a sorted array, the kth largest element is at index (n-k).\\n\\nHere's the implementation in pseudocode:\\n\\n```\\nfunction findKthLargest(nums, k):\\n    return quickSelect(nums, 0, nums.length - 1, nums.length - k)\\n\\nfunction quickSelect(nums, left, right, k):\\n    if left == right:\\n        return nums[left]\\n\\n    pivot_index = partition(nums, left, right)\\n\\n    if k == pivot_index:\\n        return nums[k]\\n    else if k < pivot_index:\\n        return quickSelect(nums, left, pivot_index - 1, k)\\n    else:\\n        return quickSelect(nums, pivot_index + 1, right, k)\\n\\nfunction partition(nums, left, right):\\n    pivot = nums[right]\\n    i = left\\n    for j from left to right - 1:\\n        if nums[j] <= pivot:\\n            swap nums[i] and nums[j]\\n            i++\\n    swap nums[i] and nums[right]\\n    return i\\n```\\n\\nAnalysis:\\n- Time Complexity: O(n) on average, O(n^2) in the worst case. The worst case occurs when the pivot is always the smallest or largest element, leading to unbalanced partitioning. However, with a good choice of pivot (e.g., a random pivot), the expected time complexity is O(n).\\n- Space Complexity: O(1) for an in-place implementation. The recursion could potentially use O(log n) to O(n) stack space depending on the pivot selection, but we can use a randomized pivot to make the recursion depth O(log n) with high probability.\\n\\nWhy it's optimal:\\n1. O(n) is the optimal time complexity for this problem. This is because any algorithm to find the kth largest element must at least examine all n elements of the array, leading to a lower bound of O(n) on the time complexity.\\n2. The QuickSelect algorithm achieves this O(n) average time complexity, making it optimal in the average case.\\n3. The worst-case time complexity of O(n^2) can be improved to O(n) by using a technique called the median of medians to select the pivot, but this adds a lot of complexity to the algorithm and is rarely used in practice due to its large constant factors.\\n\\nIn practice, the randomized QuickSelect algorithm is the go-to solution for finding the kth largest element in an unsorted array, given its simplicity and optimal expected time complexity.\\n\\nActually, I now realize that we can simplify the implementation a bit by directly passing in the (n-k)th index to the quickSelect function. Let me re-write the pseudocode:\\n\\n```\\nfunction findKthLargest(nums, k):\\n    return quickSelect(nums, 0, nums.length - 1, nums.length - k)\\n\\nfunction quickSelect(nums, left, right, k_index):\\n    if left == right:\\n        return nums[left]\\n\\n    pivot_index = partition(nums, left, right)\\n\\n    if k_index == pivot_index:\\n        return nums[k_index]\\n    else if k_index < pivot_index:\\n        return quickSelect(nums, left, pivot_index - 1, k_index)\\n    else:\\n        return quickSelect(nums, pivot_index + 1, right, k_index)\\n\\nfunction partition(nums, left, right):\\n    // Random pivot for better average performance\\n    random_index = random integer between left and right (inclusive)\\n    swap nums[random_index] and nums[right]\\n    \\n    pivot = nums[right]\\n    i = left\\n    for j from left to right - 1:\\n        if nums[j] <= pivot:\\n            swap nums[i] and nums[j]\\n            i++\\n    swap nums[i] and nums[right]\\n    return i\\n```\\n\\nThis implementation uses a randomized pivot to ensure good average-case performance.\\n\\nTo conclude, the QuickSelect algorithm is the optimal solution for finding the kth largest element in an unsorted array, with an expected time complexity of O(n) and a space complexity of O(1).\", 'type': 'thinking'}, {'text': \"# Finding the Kth Largest Element: Optimal Solution\\n\\n## Problem Understanding\\nWe need to find the kth largest element in an unsorted array with optimal time complexity.\\n\\n## Optimal Approach: QuickSelect Algorithm\\nThe most efficient solution is the QuickSelect algorithm, which is based on the partition scheme from QuickSort.\\n\\n```python\\ndef findKthLargest(nums, k):\\n    return quickSelect(nums, 0, len(nums)-1, len(nums)-k)\\n    \\ndef quickSelect(nums, left, right, k_index):\\n    if left == right:\\n        return nums[left]\\n        \\n    # Choose a random pivot for better average case performance\\n    pivot_idx = partition(nums, left, right)\\n    \\n    if k_index == pivot_idx:\\n        return nums[k_index]\\n    elif k_index < pivot_idx:\\n        return quickSelect(nums, left, pivot_idx-1, k_index)\\n    else:\\n        return quickSelect(nums, pivot_idx+1, right, k_index)\\n        \\ndef partition(nums, left, right):\\n    # For randomized pivot (better average performance)\\n    import random\\n    pivot_idx = random.randint(left, right)\\n    nums[pivot_idx], nums[right] = nums[right], nums[pivot_idx]\\n    \\n    pivot = nums[right]\\n    i = left\\n    for j in range(left, right):\\n        if nums[j] <= pivot:\\n            nums[i], nums[j] = nums[j], nums[i]\\n            i += 1\\n    nums[i], nums[right] = nums[right], nums[i]\\n    return i\\n```\\n\\n## Complexity Analysis\\n\\n### Time Complexity\\n- **Average case**: O(n)\\n- **Worst case**: O(n²) - occurs with consistently bad pivot choices\\n- The randomized pivot selection makes the worst case very unlikely in practice\\n\\n### Space Complexity\\n- O(1) for the in-place implementation\\n- O(log n) average case recursion stack space\\n\\n## Why This Solution is Optimal\\n\\n1. **Lower bound proof**: Any comparison-based algorithm must examine all n elements at least once to find the kth largest, giving a theoretical lower bound of Ω(n).\\n\\n2. **Alternative approaches comparison**:\\n   - Sorting: O(n log n) time - sub-optimal\\n   - Heap-based: O(n + k log n) time - sub-optimal for small k\\n   - QuickSelect: O(n) average time - matches the theoretical lower bound\\n\\n3. **Further optimizations**: While the median-of-medians pivot selection technique can guarantee O(n) worst-case time, it's rarely used in practice due to high constant factors.\\n\\nThe randomized QuickSelect algorithm achieves the optimal O(n) expected time complexity while maintaining simplicity and practical efficiency.\", 'type': 'text'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "\n",
    "# Create a template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an experienced programmer and mathematicalanalyst.\"),\n",
    "    (\"user\", \"{problem}\")\n",
    "])\n",
    "\n",
    "# Anthropic client\n",
    "anthropicClient = ChatAnthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "model_name=\"claude-3-7-sonnet-20250219\", \n",
    "thinking={\"type\": \"enabled\", \"budget_tokens\": 10000},\n",
    "max_tokens=60_000\n",
    ")\n",
    "\n",
    "# Create and run a chain\n",
    "chain = template | anthropicClient\n",
    "\n",
    "# Complex algorithmic problem\n",
    "problem = \"\"\"\n",
    "Design an algorithm to find the kth largest element in an unsorted array\n",
    "with the optimal time complexity. Analyze the time and space complexity\n",
    "of your solution and explain why it's optimal.\n",
    "\"\"\"\n",
    "\n",
    "response = anthropicClient.invoke([HumanMessage(content=problem)])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29d928",
   "metadata": {},
   "source": [
    "# Prompt Template \n",
    " prompt template helps to translate user inputs and parameters to instructions to language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c211d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Alice! How can I assist you today?\n",
      "System: You are a helpful assistant.\n",
      "Human: What is the capital of India?\n",
      "input_variables=[] input_types={} partial_variables={} messages=[SystemMessage(content='You are a knowledgeable assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the theory of relativity in simple terms.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Example 1: Simple Prompt Template\n",
    "simple_prompt = PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"Hello, {name}! How can I assist you today?\"\n",
    ")\n",
    "print(simple_prompt.format(name=\"Alice\"))\n",
    "\n",
    "# Example 2: Chat Prompt Template with System and Human Messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"What is the capital of {country}?\")\n",
    "])\n",
    "\n",
    "formatted_prompt = chat_prompt.format(country=\"India\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "# Example 3: Using HumanMessage and SystemMessage explicitly\n",
    "chat_prompt_explicit = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a knowledgeable assistant.\"),\n",
    "    HumanMessage(content=\"Explain the theory of relativity in simple terms.\")\n",
    "])\n",
    "\n",
    "print(chat_prompt_explicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3395d60",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL) \n",
    "Rather than focusing on how to execute each step, LCEL lets you define what you want to accomplish, allowing LangChain to handle the execution details behind the scenes.\n",
    "\n",
    "\n",
    "Runnable interface is at the core of LCEL, every component built with LCEL adheres to this interface which provides below methods . <br>\n",
    "invoke() - Processes a single input synchronously and returns an output<br>\n",
    "stream() - Streams output as it’s being generated <br>\n",
    "batch() - Efficiently processes multiple inputs in parallel <br>\n",
    "ainvoke(), abatch(), astream(): Asynchronous versions of the above methods.\n",
    "\n",
    "\n",
    "## what happens intenally when this chain in invoked??\n",
    "As each component implements the Runnable Interface all of them have the method invoke().\n",
    "Each component calls the their invoke() method and pass their output as input to other component.\n",
    "\n",
    "Flow\n",
    "Input → [Runnable1.invoke()] → Result1 → [Runnable2.invoke()] → Result2 → [Runnable3.invoke()] → Final Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d8836",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d50a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode? \n",
      "Because the light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create components\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain them together using LCEL\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Use the chain\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be40989",
   "metadata": {},
   "source": [
    "### chaining the chain for the complex expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f615363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mood of this story is positive, uplifting, and inspirational. The serene and peaceful setting of the town, combined with Lily's fascination with the sky, creates a sense of wonder and beauty. The appearance of the mysterious figure adds an element of magic and awe to the story, while the task given to Lily to spread love and kindness brings a sense of hope and purpose. As Lily helps to transform the town and bring its people together, the mood becomes one of joy, fulfillment, and gratitude. Overall, the story conveys a sense of goodness and light, leaving the reader with a feeling of warmth and positivity.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\"Analyze the following story's mood:\\n{story}\")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "\n",
    "# Combine chains\n",
    "# story_with_analysis = story_chain | analysis_chain\n",
    "story_with_analysis = story_chain | analysis_chain\n",
    "\n",
    "# Run the combined chain\n",
    "result = story_with_analysis.invoke({\"topic\": \" The Sky\"})\n",
    "# print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fdae8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: a rainy day\n",
      "story: It was a cool, dreary morning as the raindrops tapped against the window pane. Sarah sighed as she looked outside, feeling disappointed that her plans for a day at the park with her friends had been washed away by the relentless downpour. \n",
      "\n",
      "As she sat in her cozy living room, the sound of the rain became a comforting background noise. She decided to make the most of the rainy day by curling up on the couch with a good book and a cup of hot tea. The pages of the book fluttered in her hands as she lost herself in a world of fantasy and adventure.\n",
      "\n",
      "Outside, the rain continued to fall, creating a soothing rhythm that seemed to lull her into a state of relaxation. The soft pitter-patter of the rain on the roof created a peaceful atmosphere, and Sarah found herself feeling grateful for the excuse to slow down and enjoy a quiet day at home.\n",
      "\n",
      "As the day turned into evening, the rain began to taper off, leaving behind a glistening sheen on the pavement. Sarah stepped outside for a moment, taking in the fresh, clean scent of the rain-washed air. The world seemed refreshed and rejuvenated.\n",
      "\n",
      "As she headed back inside, Sarah realized that sometimes a rainy day could be just what she needed - a chance to unwind, reflect, and appreciate the simple pleasures of life. She smiled, feeling content and grateful for the cozy day spent indoors, listening to the rain.\n",
      "analysis: The mood of this story can be described as cozy, peaceful, and reflective. The rainy day creates a sense of tranquility and relaxation, with the sound of the rain providing a soothing backdrop. The protagonist, Sarah, finds solace in the simple pleasures of curling up with a book and tea, and appreciates the opportunity to slow down and enjoy a quiet day at home. The description of the rain-washed world outside and the refreshed air contribute to the overall mood of contentment and gratitude. The story conveys a sense of finding beauty in the mundane and finding joy in the little moments of life.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced output formatting keepig the original story context\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import os\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\"Analyze the following story's mood:\\n{story}\")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "\n",
    "enhanced_chain = RunnablePassthrough.assign(\n",
    "story=story_chain # Add 'story' key with generated content \n",
    ").assign(\n",
    "analysis=analysis_chain # Add 'analysis' key with analysis of the story\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "result = enhanced_chain.invoke({\"topic\": \"a rainy day\"})\n",
    "#print(result.keys())\n",
    "# print keys and values separately\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd6b3e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture is a deep learning model designed for natural\n",
      "language processing tasks. It uses self-attention to allow each word in a\n",
      "sentence to attend to every other word, capturing long-range dependencies. The\n",
      "model consists of an encoder and a decoder, each with layers of self-attention\n",
      "and feed-forward neural networks. Its parallelization capabilities make it\n",
      "faster and more efficient than traditional models like RNNs and LSTMs. The\n",
      "Transformer architecture has achieved state-of-the-art performance in tasks like\n",
      "machine translation, text summarization, and question answering, and has been\n",
      "applied in other domains as well. It represents a significant advancement in\n",
      "deep learning for sequential data processing and has enabled further innovation\n",
      "in natural language processing.   Flow diagram: Input (sentence) -> Encoder\n",
      "(self-attention + feed-forward NN) -> Decoder (self-attention + feed-forward NN)\n",
      "-> Output (translated sentence)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Do a good research about the concept {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\"Analyze and make it simple to understand for software tech audience give the flow diagram like information :\\n{story}\")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "\n",
    "# Combine chains\n",
    "# story_with_analysis = story_chain | analysis_chain\n",
    "story_with_analysis = story_chain | analysis_chain\n",
    "\n",
    "# Run the combined chain\n",
    "result = story_with_analysis.invoke({\"topic\": \" The transformer architecture\"})\n",
    "# print the result\n",
    "#print(result)\n",
    "import textwrap\n",
    "print(textwrap.fill(result, width=80))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
